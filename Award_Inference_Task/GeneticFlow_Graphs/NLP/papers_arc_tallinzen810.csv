2021.scil-1.12,"Structure Here, Bias There: Hierarchical Generalization by Jointly Learning Syntactic Transformations",2021,-1,-1,3,0.0,2214,karl mulligan,Proceedings of the Society for Computation in Linguistics 2021,0,0,0.0,0.0
2021.findings-emnlp.421,Does Putting a Linguist in the Loop Improve {NLU} Data Collection?,2021,-1,-1,9,0.0,7472,alicia parrish,Findings of the Association for Computational Linguistics: EMNLP 2021,0,0,0.0,0.0
2021.emnlp-main.72,Frequency Effects on Syntactic Rule Learning in Transformers,2021,-1,-1,3,0.0,4299,jason wei,Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing,0,0,0.0,0.0
2021.conll-1.15,Counterfactual Interventions Reveal the Causal Effect of Relative Clause Representations on Agreement Prediction,2021,-1,-1,3,0.0,4304,shauli ravfogel,Proceedings of the 25th Conference on Computational Natural Language Learning,0,0,0.0,0.0
2021.conll-1.28,{NOPE}: A Corpus of Naturally-Occurring Presuppositions in {E}nglish,2021,-1,-1,8,0.0,7472,alicia parrish,Proceedings of the 25th Conference on Computational Natural Language Learning,0,0,0.0,0.0
2021.blackboxnlp-1.4,The Language Model Understood the Prompt was Ambiguous: Probing Syntactic Uncertainty Through Generation,2021,-1,-1,2,0.0,11381,laura aina,Proceedings of the Fourth BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP,0,0,0.0,0.0
2021.acl-long.144,Causal Analysis of Syntactic Agreement Mechanisms in Neural Language Models,2021,-1,-1,5,0.0,12904,matthew finlayson,Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers),1,0,0.0,0.0
2020.tacl-1.9,Does Syntax Need to Grow on Trees? Sources of Hierarchical Inductive Bias in Sequence-to-Sequence Networks,2020,46,1,3,0.0,5076,thomas mccoy,Transactions of the Association for Computational Linguistics,0,0,1.0351692968226567,-0.22840386482309372
2020.scil-1.6,Neural network learning of the {R}ussian genitive of negation: optionality and structure sensitivity,2020,0,0,2,0.0,15505,natalia talmina,Proceedings of the Society for Computation in Linguistics 2020,0,0,2.1256310135074585,-0.2786421345301324
2020.scil-1.34,Tensor Product Decomposition Networks: Uncovering Representations of Structure Learned by Neural Networks,2020,0,0,2,0.0,5076,thomas mccoy,Proceedings of the Society for Computation in Linguistics 2020,0,0,-1.154429598881078,2.145121840803349
2020.emnlp-main.731,{COGS}: A Compositional Generalization Challenge Based on Semantic Interpretation,2020,-1,-1,2,0.0,2284,najoung kim,Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP),0,0,0.0,0.0
2020.blackboxnlp-1.21,{BERT}s of a feather do not generalize together: Large variability in generalization across models with similar test set performance,2020,-1,-1,3,0.0,5076,thomas mccoy,Proceedings of the Third BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP,0,0,0.0,0.0
2020.blackboxnlp-1.23,Discovering the Compositional Structure of Vector Representations with Role Learning Networks,2020,-1,-1,3,0.0,4374,paul soulos,Proceedings of the Third BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP,0,0,0.0,0.0
2020.acl-main.212,Syntactic Data Augmentation Increases Robustness to Inference Heuristics,2020,29,2,5,0.0,22221,junghyun min,Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics,1,0,1.376812664370322,-0.2411125520013037
2020.acl-main.303,Representations of Syntax {[MASK]} Useful: {E}ffects of Constituency and Dependency Structure in Recursive {LSTM}s,2020,34,0,2,0.0,21232,michael lepori,Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics,1,0,0.8158627556593956,-0.30072557380669124
2020.acl-main.465,How Can We Accelerate Progress Towards Human-like Linguistic Generalization?,2020,43,0,1,1.0,2216,tal linzen,Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics,1,0,0.9287743246200872,0.6683759712277104
2020.acl-main.490,Cross-Linguistic Syntactic Evaluation of Word Prediction Models,2020,40,0,5,0.571429,3971,aaron mueller,Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics,1,0,0.34747256346818045,-0.32775154699461384
W19-0101,Can Entropy Explain Successor Surprisal Effects in Reading?,2019,-1,-1,2,0.0,8212,marten schijndel,Proceedings of the Society for Computation in Linguistics ({SC}i{L}) 2019,0,0,0.0,0.0
S19-1026,Probing What Different {NLP} Tasks Teach Machines about Function Word Comprehension,2019,0,16,9,0.0,2284,najoung kim,Proceedings of the Eighth Joint Conference on Lexical and Computational Semantics (*{SEM} 2019),0,0,1.1097882879806726,-0.2866979934203241
P19-1334,Right for the Wrong Reasons: Diagnosing Syntactic Heuristics in Natural Language Inference,2019,47,24,3,0.0,25232,tom mccoy,Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics,1,0,0.9000969921955455,0.07248527210310247
N19-1356,Studying the Inductive Biases of {RNN}s with Synthetic Variations of Natural Languages,2019,30,0,3,0.0,4304,shauli ravfogel,"Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",0,0,0.36776316203156717,-1.1312913873718158
K19-1007,Using Priming to Uncover the Organization of Syntactic Representations in Neural Language Models,2019,32,1,3,0.0,4226,grusha prasad,Proceedings of the 23rd Conference on Computational Natural Language Learning (CoNLL),0,0,0.9389206532369093,-0.25579296725566797
D19-1592,Quantity doesn{'}t buy quality syntax with neural language models,2019,0,6,3,0.0,8212,marten schijndel,Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),0,0,0.33282133964297705,-0.14938062939485172
W18-0104,Phonological (un)certainty weights lexical activation,2018,-1,-1,4,0.0,28695,laura gwilliams,Proceedings of the 8th Workshop on Cognitive Modeling and Computational Linguistics ({CMCL} 2018),0,0,0.0,0.0
N18-1108,Colorless Green Recurrent Networks Dream Hierarchically,2018,31,10,4,0.0,22877,kristina gulordava,"Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)",0,0,0.4043345113628443,-0.01371081401644901
D18-1151,Targeted Syntactic Evaluation of Language Models,2018,0,55,2,0.0,8316,rebecca marvin,Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing,0,0,0.33282133964297705,-0.14938062939485172
D18-1499,A Neural Model of Adaptation in Reading,2018,29,1,2,0.0,8212,marten schijndel,Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing,0,0,0.20940368854954766,-0.03761001850590624
K17-1003,Exploring the Syntactic Abilities of {RNN}s with Multi-task Learning,2017,20,11,3,0.0,28675,emile enguehard,Proceedings of the 21st Conference on Computational Natural Language Learning ({C}o{NLL} 2017),0,0,0.5395521994207387,-0.1657942191061485
E17-2020,Comparing Character-level Neural Language Models Using a Lexical Decision Task,2017,20,2,2,0.0,32977,gael godais,"Proceedings of the 15th Conference of the {E}uropean Chapter of the Association for Computational Linguistics: Volume 2, Short Papers",0,0,-0.05600143163418515,-0.07450354818285555
W16-2503,Issues in evaluating semantic spaces using word analogies,2016,15,28,1,1.0,2216,tal linzen,Proceedings of the 1st Workshop on Evaluating Vector-Space Representations for {NLP},0,0,-0.031362483355824,-0.032453646393453
W16-2513,Evaluating vector space models using human semantic priming results,2016,23,3,2,0.0,8001,allyson ettinger,Proceedings of the 1st Workshop on Evaluating Vector-Space Representations for {NLP},0,0,-0.18202329447879134,-0.37287726652617836
S16-2001,Quantificational features in distributional word representations,2016,21,3,1,1.0,2216,tal linzen,Proceedings of the Fifth Joint Conference on Lexical and Computational Semantics,0,0,-0.031362483355824,-0.032453646393453
Q16-1037,Assessing the Ability of {LSTM}s to Learn Syntax-Sensitive Dependencies,2016,17,166,1,1.0,2216,tal linzen,Transactions of the Association for Computational Linguistics,0,0,1.0114317668609931,-0.740584640788024
D15-1134,A model of rapid phonotactic generalization,2015,17,2,1,1.0,2216,tal linzen,Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing,0,0,-0.8612265606292914,0.001877471884422971
W14-2002,Investigating the role of entropy in sentence processing,2014,19,5,1,1.0,2216,tal linzen,Proceedings of the Fifth Workshop on Cognitive Modeling and Computational Linguistics,0,0,0.9751357152033668,-0.4586193020769947
